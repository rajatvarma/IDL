{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rajat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgs = np.load('75/images.npy')\n",
    "imgs = imgs.astype('float32')\n",
    "indices = np.random.permutation(imgs.shape[0])\n",
    "imgs = imgs[indices]\n",
    "\n",
    "split = int(18000*0.7)\n",
    "\n",
    "train_imgs = imgs[100:split]\n",
    "test_imgs = imgs[split:]\n",
    "val_imgs = imgs[0:100]\n",
    "\n",
    "train_imgs = train_imgs / 255.0\n",
    "test_imgs = test_imgs / 255.0\n",
    "val_imgs = val_imgs / 255.0\n",
    "\n",
    "labels = np.load('75/labels.npy')\n",
    "labels = labels.astype('int32')\n",
    "labels = labels[indices]\n",
    "train_labels = labels[100:split]\n",
    "test_labels = labels[split:]\n",
    "val_labels = labels[0:100]\n",
    "\n",
    "train_imgs = train_imgs.reshape((train_imgs.shape[0], 75, 75, 1))\n",
    "val_imgs = val_imgs.reshape((val_imgs.shape[0], 75, 75, 1))\n",
    "test_imgs = test_imgs.reshape((test_imgs.shape[0], 75, 75, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_time(time):\n",
    "    ntime = 0\n",
    "    if time[1] > 30:\n",
    "        ntime = (time[0] + 0.5)\n",
    "    else:\n",
    "        ntime = time[0]\n",
    "    return ntime\n",
    "\n",
    "train_labels_converted = np.array([conv_time(time) for time in train_labels])\n",
    "test_labels_converted = np.array([conv_time(time) for time in test_labels])\n",
    "val_labels_converted = np.array([conv_time(time) for time in val_labels])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "test_labels_encoded = encoder.fit_transform(test_labels_converted.reshape(-1))\n",
    "train_labels_encoded = encoder.fit_transform(train_labels_converted.reshape(-1))\n",
    "val_labels_encoded = encoder.fit_transform(val_labels_converted.reshape(-1))\n",
    "\n",
    "OHencoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_oh = OHencoder.fit_transform(train_labels_encoded.reshape(-1, 1))\n",
    "val_labels_oh = OHencoder.fit_transform(val_labels_encoded.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 75, 75, 1) (12500, 24)\n",
      "Epoch 1/10\n",
      "49/49 [==============================] - 7s 106ms/step - loss: 3.1781 - accuracy: 0.0438 - val_loss: 3.1790 - val_accuracy: 0.0200\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 5s 103ms/step - loss: 3.1779 - accuracy: 0.0445 - val_loss: 3.1801 - val_accuracy: 0.0200\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 5s 104ms/step - loss: 3.1781 - accuracy: 0.0445 - val_loss: 3.1788 - val_accuracy: 0.0200\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 5s 105ms/step - loss: 3.1779 - accuracy: 0.0445 - val_loss: 3.1795 - val_accuracy: 0.0200\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 5s 105ms/step - loss: 3.1778 - accuracy: 0.0445 - val_loss: 3.1803 - val_accuracy: 0.0200\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 5s 108ms/step - loss: 3.1777 - accuracy: 0.0445 - val_loss: 3.1802 - val_accuracy: 0.0200\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 6s 126ms/step - loss: 3.1777 - accuracy: 0.0445 - val_loss: 3.1799 - val_accuracy: 0.0200\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 6s 123ms/step - loss: 3.1777 - accuracy: 0.0445 - val_loss: 3.1793 - val_accuracy: 0.0200\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 6s 116ms/step - loss: 3.1776 - accuracy: 0.0445 - val_loss: 3.1798 - val_accuracy: 0.0200\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 5s 111ms/step - loss: 3.1772 - accuracy: 0.0445 - val_loss: 3.1819 - val_accuracy: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e623c4810>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (75, 75, 1)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(kernel_size=(5,5), strides = (2,2), activation=\"relu\", filters=32))\n",
    "model.add(keras.layers.Conv2D(activation=\"relu\", filters=32, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(keras.layers.Conv2D(kernel_size=(3,3), activation=\"relu\", filters=32))\n",
    "model.add(keras.layers.Conv2D(kernel_size=(3,3), activation=\"relu\", filters=32))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(keras.layers.Conv2D(kernel_size=(3,3), activation=\"relu\", filters=64))\n",
    "model.add(keras.layers.Conv2D(kernel_size=(3,3), activation=\"relu\", filters=64))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(units=512, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=512, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=32, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=24, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "print(train_imgs.shape, train_labels_oh.shape)\n",
    "val_labels_oh = val_labels_oh.reshape((val_labels_oh.shape[0], 24))\n",
    "\n",
    "model.fit(train_imgs, train_labels_oh, epochs=10, batch_size=256, validation_data=(val_imgs, val_labels_oh), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 1s 6ms/step\n",
      "4.018518518518518 %\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_imgs)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "\n",
    "results = encoder.inverse_transform(preds)\n",
    "\n",
    "accuracy = np.sum(results == test_labels_converted) / len(test_labels_converted)\n",
    "print(accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ttm_reg \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m      2\u001b[0m ttm_reg\u001b[38;5;241m.\u001b[39madd(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), strides \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m),input_shape\u001b[38;5;241m=\u001b[39minput_shape))\n\u001b[0;32m      3\u001b[0m ttm_reg\u001b[38;5;241m.\u001b[39madd(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPooling2D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "ttm_reg = keras.models.Sequential()\n",
    "ttm_reg.add(keras.layers.Conv2D(activation='relu', filters=32, kernel_size=(3,3), strides = (2,2),input_shape=input_shape))\n",
    "ttm_reg.add(keras.layers.MaxPooling2D(pool_size=2))\n",
    "ttm_reg.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n",
    "ttm_reg.add(keras.layers.Conv2D(filters=32 ,kernel_size=(3,3), activation='relu'))\n",
    "ttm_reg.add(keras.layers.MaxPooling2D(pool_size=2))\n",
    "ttm_reg.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "ttm_reg.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "\n",
    "\n",
    "ttm_reg.add(keras.layers.Flatten())\n",
    "ttm_reg.add(keras.layers.Dense(units=256, activation='relu'))\n",
    "ttm_reg.add(keras.layers.Dense(units=256, activation='relu'))\n",
    "ttm_reg.add(keras.layers.Dense(units=256, activation='relu'))\n",
    "ttm_reg.add(keras.layers.Dense(units=256, activation='relu'))\n",
    "ttm_reg.add(keras.layers.Dropout(0.1))\n",
    "ttm_reg.add(keras.layers.Dense(units=128, activation='relu'))\n",
    "ttm_reg.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "ttm_reg.add(keras.layers.Dense(units=1, activation=\"softplus\"))\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 10)\n",
    "ttm_reg.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])\n",
    "ttm_reg.fit(train_imgs, y_train_ttm_reg, epochs=40, batch_size = 512, validation_data = (valid_images,y_valid_ttm_reg), callbacks = [early_stop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
